!pip install -q transformers accelerate

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

# Load dataset
df = pd.read_csv("/content/drive/MyDrive/paragraph_split(6).csv")

# Filter rows where paragraph is not generated yet
df = df[df["generated"] == 0]

# Count unique titles in the filtered dataset
unique_titles = df["title"].unique()
print("Number of titles with generated == 0:", len(unique_titles))

from google.colab import drive
drive.mount('/content/drive')

import os
import pandas as pd
import random
from tqdm import tqdm
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# =====================
# Path configuration
# =====================
DATA_PATH = "/content/drive/MyDrive/paragraph_split(6).csv"
BACKUP_PATH = "/content/drive/MyDrive/generated_backup(5).csv"
FINAL_OUTPUT = "/content/drive/MyDrive/final_generated.csv"

# Title selection range (e.g., process 10,000 titles starting from index 70010)
START_INDEX = 70010
NUM_TITLES = 10000

# =====================
# Load the language model
# =====================
model_name = "kakaocorp/kanana-1.5-2.1b-instruct-2505"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)
model.eval()
generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

# =====================
# Step 1: Load dataset
# =====================
df = pd.read_csv(DATA_PATH)

# Step 2: Keep only non-generated paragraphs
df = df[df["generated"] == 0]

# Step 3: Extract ordered list of unique titles from the filtered dataset
ordered_titles = df["title"].tolist()  # keep order, allow duplicates
unique_ordered_titles = []
seen = set()
for t in ordered_titles:
    if t not in seen:
        seen.add(t)
        unique_ordered_titles.append(t)

# Select target titles within the given range
target_titles = unique_ordered_titles[START_INDEX:START_INDEX + NUM_TITLES]

# Step 4: Filter dataset for the selected titles only
df = df[df["title"].isin(target_titles)].copy()
remaining_titles = list(set(target_titles))

generated_data = []

# =====================
# Step 5: Paragraph generation loop
# =====================
for title in tqdm(remaining_titles, desc="Generating paragraphs"):
    group = df[df["title"] == title].reset_index(drop=True)
    if len(group) < 3:
        continue

    # Candidate positions for inserting a middle paragraph
    candidates = [i for i in range(1, len(group) - 1)]
    if not candidates:
        continue

    middle_idx = random.choice(candidates)
    before = group.loc[middle_idx - 1, "paragraph_text"]
    after = group.loc[middle_idx + 1, "paragraph_text"]

    # Prompt design
    prompt = (
        "Below are the preceding and following paragraphs from a Wikipedia article. "
        "Write a paragraph that naturally connects these two, without repeating content from the original paragraphs, "
        "and expand on the context logically.\n"
        "- Ensure logical flow and coherence.\n"
        "- Avoid conversational tone, questions, or bullet lists. Write in complete sentences.\n"
        "- The paragraph should contain 2–6 sentences.\n"
        "- Write as a single paragraph only.\n\n"
        f"Previous paragraph:\n{before}\n\n"
        f"Next paragraph:\n{after}\n\n"
        "Answer:"
    )

    try:
        with torch.inference_mode():
            output = generator(
                prompt,
                max_new_tokens=120,
                do_sample=True,
                top_k=50,
                top_p=0.85,
                temperature=0.8
            )[0]["generated_text"]

        # Extract only the newly generated paragraph
        final_output = output[len(prompt):].strip()

        new_row = {
            "title": title,
            "paragraph_text": final_output,
            "generated": 1,
            "para_index": group.loc[middle_idx, "para_index"]
        }

        # Save to backup file in real-time to avoid data loss
        pd.DataFrame([new_row]).to_csv(
            BACKUP_PATH, mode="a", index=False,
            header=not os.path.exists(BACKUP_PATH),
            encoding="utf-8-sig"
        )

        generated_data.append(new_row)

    except Exception as e:
        print(f"[❌ ERROR] {title} - {e}")
        continue

# =====================
# Step 6: Save final generated paragraphs
# =====================
pd.DataFrame(generated_data).to_csv(FINAL_OUTPUT, index=False, encoding="utf-8-sig")
