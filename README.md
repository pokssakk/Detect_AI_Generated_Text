# Detect LLM: Paragraph-level Classification of AI-Generated Text
<img width="1749" height="1256" alt="image" src="https://github.com/user-attachments/assets/53d6584f-be0f-4f14-8584-37c6678d754d" />


## :round_pushpin: Overview
This repository presents our solution for the **Detect AI-generated Text Competition**, a national-level data science competition hosted in 2025, Dacon.

The goal of the cahllenge was to detect whether a **paragraph** was written by a Large Language Model (LLM), with only document-level labels provided. This required re-egineering the data, designing weak supervision strategies, and buildig robust classifiers in a highly imbalanced setting.

[Competition Link](https://dacon.io/competitions/official/236473/overview/description)

<br>

## :round_pushpin: Competition Information

- **Host**: Dacon
- **Track**: Detect AI-generated Text
- **Evaluation Metric**: ROC-AUC
- **Input**: Document-level 'full_text' (Provided as train.csv)
- **Label**: 'generated' (0 for human-written, 1 for AI-generated)
- **Goal**: Classify each paragraph as human- or AI-generated

### âœ”ï¸ train.csv
|title|full_text|generated|
|------|---|---|
|ì¹´í˜¸ì˜¬ë¼ì›¨ì„¬|ì¹´í˜¸ì˜¬ë¼ì›¨ì„¬ì€ í•˜ì™€ì´ ì œë„ë¥¼ êµ¬ì„±í•˜ëŠ” (ì¤‘ëµ...) |0|
|ì²­ìƒ‰ê±°ì„±|ì²œë¬¸í•™ì—ì„œ ì²­ìƒ‰ê±°ì„±ì€ ê´‘ë„ ë¶„ë¥˜ (ì¤‘ëµ...) |0|
|ìˆ˜ë‚œê³¡|ìˆ˜ë‚œê³¡ì€ ë°°ìš°ì˜ ì—°ê¸° ì—†ì´ ë¬´ëŒ€ì— (ì¤‘ëµ...) |1|

### âœ”ï¸ test.csv
|ID|title|paragraph_index|paragraph_text|
|---|------|---|---|
|TEST_0000|ê³µì¤‘ ë„ë•ì˜ ì˜ì˜ì™€ í•„ìš”ì„±|0|ë„ë•ì´ë€ ì›ë˜ ê°œì¸ì˜ ìê°...|
|TEST_0001|ê³µì¤‘ ë„ë•ì˜ ì˜ì˜ì™€ í•„ìš”ì„±|1|ë„ë•ì€ ë‹¨ìˆœíˆ ê°œì¸ì˜ ë¬¸ì œ...|
|TEST_0002|ê³µì¤‘ ë„ë•ì˜ ì˜ì˜ì™€ í•„ìš”ì„±|2|ì—¬ê¸°ì— ì´ë¥¸ë°” ê³µì¤‘ë„ë•ì€...|

### âœ”ï¸ sample_submission.csv
|ID|generated|
|---|------|
|TEST_0000|0|
|TEST_0001|0|

<br>

## :round_pushpin: Data Analysis & Preprocessing

Our core challenge was to reconstruct paragraph-level labels from document-level data in an extremely imbalanced and noisy setting. To overcome this, we developed multiple **weak supervision and filtering strategies**, focusing on data-centric approaches.

<br>
<p align='center'>
<img width="699" height="268" alt="image" src="https://github.com/user-attachments/assets/0048a66a-171c-4ca7-a485-4e4a78ac55ce" />

### â¡ï¸ 1) Data Restructuring
- Each `full_text` (up to 9 paragraphs) was split into multiple `paragraph_text` units
- For long paragraphs, we applied **sliding window chunking** with overlapping stride
- Input format:  
  `"ì œëª©: {title} ë³¸ë¬¸: {paragraph_text}"`

### â¡ï¸ 2) Class Imbalance Handling
The dataset exhibited **severe class imbalance**:  
approximately 10:1 ratio between `generated=0` and `generated=1` labels.

To address this, we performed both **data augmentation** and **filtering** for high-confidence positive samples.
- Oversampling + Label Propagation
- Filtering using Perplexity and Semantic Similarity

### ğŸ” 3) Data Augmentation (Generated = 1)

We used a pretrained KoGPT model ("kanana") to generate synthetic paragraphs mimicking `generated=1` style.  
These augmented paragraphs were added to the training set to enrich the positive class.

- Model: kanana LLM (fine-tuned KoGPT2)
- Prompt-based generation conditioned on style
- Manual filtering + confidence scoring

### ğŸ” 4) Weak Labeling & Filtering Strategies  [learn more->](https://github.com/pokssakk/data-experiments)

We experimented with various **heuristic and unsupervised labeling techniques**:

#### 1. Perplexity-based Filtering
- Used GPT-like models to compute **perplexity scores**
- Sentences or paragraphs with low perplexity were considered likely AI-generated
- Applied thresholding to extract `generated=1` candidates

#### 2. Style Feature + PPL Clustering
- Extracted syntactic style features (e.g., average sentence length, token diversity)
- Combined with perplexity
- Performed **KMeans or HDBSCAN** clustering to isolate AI-like patterns

#### 3. HDBSCAN on Paragraph-level PPL
- Applied sentence-wise perplexity estimation
- Clustered using **HDBSCAN** to extract dense positive clusters
- Label assigned to entire paragraph if one or more sentences clustered as AI

#### 4. Perturbation-based Confidence Scoring
- Modified (perturbed) the sentence input
- Measured change in log-likelihood (LL) or model confidence
- Used as a proxy for model â€œsurpriseâ€ or fluency robustness


<br>

## :round_pushpin: Modeling Strategy

### â¡ï¸ Base Models:
The first baseline model used KLUE-RoBERTa (Which you can find and use in hugging face). The model with the best performance in STS.
|Model|STS|TC|
|------|---|---|
|mBERT-base|84.66|81.55|
|XML-R-base|89.16|83.52|
|KLUE-RoBERTa-base|92.50|85.07|

[check more ->](https://github.com/KLUE-benchmark/KLUE)

After that, many attempts were made using many models. 
[leand more ->](https://github.com/pokssakk/model-experiments)

<br>

### â¡ï¸ Key Techniques:
1. Sliding window over tokens with max pooling
2. Sentence-level Pertubation + Perplexity filtering
3. KLUE-RoBERTa embedding + cosine similarity for pseudo-labeling
4. Model ensemble using ranking + voting

<br>

### â­ Final Model:
#### 1) Data Preparation
Given the competition setting, the original labels were provided only at the Full Text level
- Even if only some paragraphs were AI-generated, the entire text was labeled as 1
- Evaluation, however, required paragraph-level AI probabilities


To address this mismatch, we processed the data as follows:

1. **Paragraph-level Relabeling (KoSimCSE + AutoEncoder)**
  - Each full_text was split into paragraph units
  - For each paragraph, we predicted whether it was likely AI-generated (1) or human-written (0) using two signals:
    - Semantic Similarity (KoSimCSE):
      - Paragraph embeddings (via BM-K/KoSimCSE-roberta-multitask) compared to the corpus mean vector
      - Low cosine similarity â†’ AI-like
    - AutoEncoder-based Reconstruction Error:
      - High AE score â†’ AI-like
  - Threshold:
    - cosine_similarity < Î¼ - 2Ïƒ or ae_score > Î¼ + 2Ïƒ â†’ labeled as generated=1
  - Title-level Correction:
    - For full_text originally labeled generated=1 but with no positive paragraphs,
      the most suspicious paragraph (highest AE score & lowest similarity) was corrected to 1

2. **Positive Class Augmentation (KANANA)**


<br>

#### 2) Models
Our final ensemble combined two fine-tuned transformer models and a CatBoost classifier:
1. **KLUE-RoBERTa-large (fine-tuned)**  
   - Base weight: `klue/roberta-large`  
   - Chosen for its strong semantic representation capability,
     which is crucial for detecting subtle contextual inconsistencies in AI-generated text
   - [Check training details here]()

2. **KLUE-RoBERTa-base (fine-tuned)**  
   - Base weight: `klue/roberta-base`  
   - Selected as a lighter and more generalizable counterpart to the large model,
     reducing overfitting risk on imbalanced data
   - [Check training details here]()

3. **CatBoost Classifier**  
   - Focused on stylometric anomalies often observed in AI-generated text, complementing semantic models  
   - Features: **unique_ratio**, **verb_ratio**, **entropy**, **polynomial interaction terms (degree=2)**, **Perplexity (PPL)** estimated via `skt/kogpt2-base-v2`  
   - [Check training details here]()

<br>

#### 3) Ensemble strategy
We adopted a custom Extreme Voting to maximize ROC-AUC:
- â‰¥2 models â‰¥0.5 â†’ max(probabilities) (optimistic consensus)
- Otherwise â†’ min(probabilities) (conservative fallback)


<br>

## :round_pushpin: Experiments & Results


<br>

## :round_pushpin: Project Structure


<br>

## :round_pushpin: Team & Contributions
Our team of 5 members divided responsibilities as follows: <br>

Team Name: **PokSSak(í­ì‹¹)**
|ì†Œì†|ì´ë¦„|ì—­í• ||
|------|---|---|---|
|ìˆ™ëª…ì—¬ìëŒ€í•™êµ ì»´í“¨í„°ì‚¬ì´ì–¸ìŠ¤ì „ê³µ(22)|ê¹€ì†Œì˜|||
|ìˆ™ëª…ì—¬ìëŒ€í•™êµ ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤ì „ê³µ(23)|ê¹€ìˆ˜ë¹ˆ|||
|ìˆ™ëª…ì—¬ìëŒ€í•™êµ ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤ì „ê³µ(23)|ì˜¤í˜„ì„œ|||
|ìˆ™ëª…ì—¬ìëŒ€í•™êµ ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤ì „ê³µ(23)|ì›ì§€ìš°|Data Engineering & Experiment Execution||
|ìˆ™ëª…ì—¬ìëŒ€í•™êµ ì†Œí”„íŠ¸ì›¨ì–´ìœµí•©ì „ê³µ(22)|ì„ì†Œì •|Data Engineering & Model Development|[sophia](https://github.com/Sophia680102)|

We collaborated for 4 weeks.


<br>

## :round_pushpin: Stacks
Environment<br>
<img src="https://img.shields.io/badge/googlecolab-F9AB00?style=for-the-badge&logo=googlecolab&logoColor=white">
<img src="https://img.shields.io/badge/github-181717?style=for-the-badge&logo=github&logoColor=white">
<img src="https://img.shields.io/badge/git-F05032?style=for-the-badge&logo=git&logoColor=white">

Development<br>
<img src="https://img.shields.io/badge/python-3776AB?style=for-the-badge&logo=python&logoColor=white">

Communication<br>
<img src="https://img.shields.io/badge/notion-000000?style=for-the-badge&logo=notion&logoColor=white">
<img src="https://img.shields.io/badge/googlemeet-00897B?style=for-the-badge&logo=googlemeet&logoColor=white">

<br>

## :round_pushpin: Key Takeaways
- 


<br>

## :round_pushpin: Appendix

- Final submission score: **ROC-AUC 0.889**
- Full competition leaderboard: *[https://dacon.io/competitions/official/236473/leaderboard]*
- PDF summary: 'docs/project_summary.pdf'
