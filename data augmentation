!pip install -q transformers accelerate

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

# Load dataset
df = pd.read_csv("/content/drive/MyDrive/paragraph_split(6).csv")

# Filter rows where paragraph is not generated yet
df = df[df["generated"] == 0]

# Count unique titles in the filtered dataset
unique_titles = df["title"].unique()
print("Number of titles with generated == 0:", len(unique_titles))

from google.colab import drive
drive.mount('/content/drive')

import os
import pandas as pd
import random
from tqdm import tqdm
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# Path configuration
DATA_PATH = "/content/drive/MyDrive/paragraph_split(6).csv"
BACKUP_PATH = "/content/drive/MyDrive/generated_backup(5).csv"
FINAL_OUTPUT = "/content/drive/MyDrive/final_generated.csv"

# Title selection range (e.g., process 10,000 titles starting from index 70010)
START_INDEX = 70010
NUM_TITLES = 10000

# Load the language model
model_name = "kakaocorp/kanana-1.5-2.1b-instruct-2505"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)
model.eval()
generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

# Step 1: Load dataset
df = pd.read_csv(DATA_PATH)

# Step 2: Keep only non-generated paragraphs
df = df[df["generated"] == 0]

# Step 3: Extract ordered list of unique titles from the filtered dataset
ordered_titles = df["title"].tolist()  # keep order, allow duplicates
unique_ordered_titles = []
seen = set()
for t in ordered_titles:
    if t not in seen:
        seen.add(t)
        unique_ordered_titles.append(t)

# Select target titles within the given range
target_titles = unique_ordered_titles[START_INDEX:START_INDEX + NUM_TITLES]

# Step 4: Filter dataset for the selected titles only
df = df[df["title"].isin(target_titles)].copy()
remaining_titles = list(set(target_titles))

generated_data = []

# Step 5: Paragraph generation loop
for title in tqdm(remaining_titles, desc="Generating paragraphs"):
    group = df[df["title"] == title].reset_index(drop=True)
    if len(group) < 3:
        continue

    # Candidate positions for inserting a middle paragraph
    candidates = [i for i in range(1, len(group) - 1)]
    if not candidates:
        continue

    middle_idx = random.choice(candidates)
    before = group.loc[middle_idx - 1, "paragraph_text"]
    after = group.loc[middle_idx + 1, "paragraph_text"]

    # Prompt design
    prompt = (
        "다음은 위키백과 문서의 앞뒤 문단입니다. 이 둘 사이에 들어갈 문단을 자연스럽게 연결하되, 기존 문단의 내용을 반복하지 말고 문맥을 확장하여 서술해 주세요.\n"
        "- 자연스럽고 논리적으로 흐르도록 문장을 연결하세요.\n"
        "- 대화체, 질문, 나열형 표현은 사용하지 말고, 반드시 완결된 문장으로 서술하세요.\n"
        "- 문단은 2~6문장 내외로 작성해주세요.\n"
        "- 단 하나의 문단으로만 작성해주세요.\n\n"
        f"앞 문단:\n{before}\n\n"
        f"뒷 문단:\n{after}\n\n"
        "답변:"
    )

    try:
        with torch.inference_mode():
            output = generator(
                prompt,
                max_new_tokens=120,
                do_sample=True,
                top_k=50,
                top_p=0.85,
                temperature=0.8
            )[0]["generated_text"]

        # Extract only the newly generated paragraph
        final_output = output[len(prompt):].strip()

        new_row = {
            "title": title,
            "paragraph_text": final_output,
            "generated": 1,
            "para_index": group.loc[middle_idx, "para_index"]
        }

        # Save to backup file in real-time to avoid data loss
        pd.DataFrame([new_row]).to_csv(
            BACKUP_PATH, mode="a", index=False,
            header=not os.path.exists(BACKUP_PATH),
            encoding="utf-8-sig"
        )

        generated_data.append(new_row)

    except Exception as e:
        print(f"[❌ ERROR] {title} - {e}")
        continue

# =====================
# Step 6: Save final generated paragraphs
# =====================
pd.DataFrame(generated_data).to_csv(FINAL_OUTPUT, index=False, encoding="utf-8-sig")
